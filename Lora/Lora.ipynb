{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb319438-0c10-44f6-890b-7a87ce31f238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc, os\n",
    "import peft\n",
    "from peft import LoraConfig, get_peft_model, AutoPeftModelForCausalLM, PeftConfig, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c4a87e-3931-49a1-8379-0ff56f1ab1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "564d0577-408a-49fb-90a0-4d5dc8220700",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 1024\n",
    "dtype = None # None for auto detection\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c52890b-e0bd-46a0-a030-26f1c6f4f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f129402-1358-4e6d-8c16-a62023e9007e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6b0530893a41618eecaa16e6447ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"meta-llama/Meta-Llama-3-8B\"\n",
    "device_map = {\"\": 0}\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL, quantization_config=bnb_config, low_cpu_mem_usage=True, device_map=\"auto\", torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30629cd0-6e11-4502-a858-9b6d0033b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data to train\n",
    "dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.hi\", split = \"train\",)\n",
    "# Use only 10% of data\n",
    "dataset = dataset.train_test_split(train_size = 0.1)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59e3a299-3226-4fc8-8591-172e99d56d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e07f04c8ff4e74aa0c3afdad505c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/16309 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=128,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs\",\n",
    "    #auto_find_batch_size=True, # Find a correct bvatch size that fits the size of Data.\n",
    "    learning_rate= 5e-5, # Higher learning rate than full fine-tuning.\n",
    "    max_steps = 240,\n",
    "    warmup_steps = 20,\n",
    "    # do_eval=True,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    seed=3047,\n",
    "    per_device_train_batch_size=1,\n",
    "    # per_device_eval_batch_size=1,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    save_steps=20,\n",
    "    # eval_steps=20,\n",
    "    logging_steps=20,\n",
    "    # gradient_accumulation_steps=4,\n",
    "    # num_train_epochs=5\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    args=training_args,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_num_proc = 2,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    packing=False\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af8cf477-5896-4376-ade9-7d49803db79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4090. Max memory = 23.41 GB.\n",
      "10.588 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96402589-22f7-4f70-81fa-a95fcb9477b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 02:20, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.406800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.523400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.343800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.091600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.267300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.258900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.159600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.266600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.244400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.558200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.310600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.267700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b351c286-3a8c-4db5-9fba-1c8091d07413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=240, training_loss=1.3082326253255208, metrics={'train_runtime': 141.3068, 'train_samples_per_second': 1.698, 'train_steps_per_second': 1.698, 'total_flos': 4292421559050240.0, 'train_loss': 1.3082326253255208, 'epoch': 0.014715801091421914})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5152eec-5de3-47d4-9ad4-159dca483af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141.3068 seconds used for training.\n",
      "2.36 minutes used for training.\n",
      "Peak reserved memory = 10.588 GB.\n",
      "Peak reserved memory for training = 0.0 GB.\n",
      "Peak reserved memory % of max memory = 45.229 %.\n",
      "Peak reserved memory for training % of max memory = 0.0 %.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "293258eb-d986-42c8-b6b8-07a08733a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "working_dir = './'\n",
    "\n",
    "output_directory = os.path.join(working_dir, \"peft_pre-outputs_llama3\")\n",
    "# peft_model_path = os.path.join(output_directory, f\"lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cd4086f-97c7-4c15-a79c-6845733998a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./peft_pre-outputs_llama3/tokenizer_config.json',\n",
       " './peft_pre-outputs_llama3/special_tokens_map.json',\n",
       " './peft_pre-outputs_llama3/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(output_directory)\n",
    "tokenizer.save_pretrained(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5358e04b-c8f6-43d7-b031-572e6de41a90",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e40178c-6bee-4895-91ca-f883d9efa6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config2 = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ada0eba-53f7-4813-860c-6b2dfd0109c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67e38f92fc5446f8b2522c216e9f410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_with_adapters_model = AutoPeftModelForCausalLM.from_pretrained(output_directory)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_directory)\n",
    "\n",
    "# loaded_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#                                         output_directory,\n",
    "#                                         torch_dtype=torch.bfloat16,\n",
    "#                                         #load_in_4bit=True,\n",
    "#                                         quantization_config=bnb_config2,\n",
    "#                                         device_map = 'cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "447ab802-1306-4cac-bc68-6beed82c8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with base model, Note that this creates a full f32 model!\n",
    "model = base_with_adapters_model.merge_and_unload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "463114ad-3420-440d-85fa-5409be7d087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.path.join(working_dir, \"peft_pre-outputs_llama3_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21f1dfde-8ea3-42a4-999a-dd01b1e6ccfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./peft_pre-outputs_llama3_full/tokenizer_config.json',\n",
       " './peft_pre-outputs_llama3_full/special_tokens_map.json',\n",
       " './peft_pre-outputs_llama3_full/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(dir)\n",
    "tokenizer.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5207e72a-0d9f-4108-9918-f504f1bd5b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function returns the outputs from the model received, and inputs.\n",
    "def get_outputs(model, inputs, max_new_tokens=100):\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        repetition_penalty=1.5, #Avoid repetition.\n",
    "        early_stopping=False, #The model can stop before reach the max_length\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "533f30cf-0f8a-45e3-a381-87b8dbf5552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9011bea9-b62a-428b-a794-ae0a75320886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21fe9f92cad41e49fc0fc9c6cec2d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load the merged model\n",
    "device_map = {\"\": 0}\n",
    "model = AutoModelForCausalLM.from_pretrained(dir, torch_dtype=dtype, quantization_config=bnb_config2, device_map=device_map)\n",
    "tokenizer = AutoTokenizer.from_pretrained(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1186c406-4469-45c8-8093-4a34e303d1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['फाइबोनैचि अनुक्रम जारी रखें: 1, 1, 2, 3, 5, 8, … (A000045). यह एक संख्या के लिए उसके, और उससे पहले आनेवाली, दो मूल गुणकों पर आधरित है।\\n\\nगणितज्ञ फ़िलिप व्हीलेर नामांकन प्रणीत करताहुए कहा, \"मैं इस आइडियाको, इसकेकोई खास रूप नहीं बनान्देख रहहूँ।\"\\n\\nइन सम्बन्धनविशेष\\nपृष्ठ शुरू करें\\nअधिकृत वर्णिमापद्-सूची\\nश्र']\n"
     ]
    }
   ],
   "source": [
    "input_sentences = tokenizer(\"फाइबोनैचि अनुक्रम जारी रखें: 1, 1, 2, 3, 5, 8,\", return_tensors=\"pt\").to('cuda')\n",
    "foundational_outputs_sentence = get_outputs(model, input_sentences, max_new_tokens=128)\n",
    "\n",
    "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9d6b76-06b9-4d2b-9427-0d23d9f23ecd",
   "metadata": {},
   "source": [
    "# FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "064958c6-c9f7-4691-a081-1f2f09a87ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae531428-1161-4838-99e2-58667f142415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "alpaca_dataset = load_dataset(\"FreedomIntelligence/alpaca-gpt4-hindi\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "617b55cb-cc74-4b96-a945-22093174dfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversations': [{'from': 'human', 'value': 'कुछ एक रीसाइक्लिंग अभियान के लिए एक नारा सुझाव दें।\\n'}, {'from': 'gpt', 'value': '1. \"ग्रीन भविष्य के लिए एक साथ: कम करें, पुन: उपयोग करें, रीसाइकल करें।\"\\n2. \"एक बेहतर कल के लिए आज ही रीसाइकल करें।\"\\n3. \"अपने कचरे को खजाना बनाएं - रीसाइकल करें!\"\\n4. \"जीवन के चक्र के लिए रीसाइकल करें।\"\\n5. \"संसाधन बचाएं, अधिक रीसाइकल करें।\"'}], 'id': '23712'}\n"
     ]
    }
   ],
   "source": [
    "print(alpaca_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24973be4-8851-4af3-a5c9-d97b4dcf2c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_hindi_prompt=\"\"\"नीचे एक निर्देश है जो किसी कार्य का वर्णन करता है, जिसे एक इनपुट के साथ जोड़ा गया है जो आगे का संदर्भ प्रदान करता है। ऐसा उत्तर लिखें जो अनुरोध को उचित रूप से पूरा करता हो।\n",
    "\n",
    "### निर्देश:\n",
    "{}\n",
    "\n",
    "### इनपुट:\n",
    "{}\n",
    "\n",
    "### प्रतिक्रिया:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e392dbf-4d85-452b-afbf-e67e4648dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def formatting_prompts_func(conversations):\n",
    "    texts = []\n",
    "    conversations = conversations[\"conversations\"]\n",
    "    for convo in conversations:\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        # Careful Aya Dataset does not have an input!\n",
    "        text = alpaca_hindi_prompt.format(convo[0][\"value\"], \"\", convo[1][\"value\"]) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "alpaca_dataset = alpaca_dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d13ad3ae-872d-48bd-a38d-fa845944091b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d937a05d755472b8abccfdee3ecfbe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/49969 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=128,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# base_model_with_new_adapter = get_peft_model(model, peft_config)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    output_dir=\"outputs_instrc\",\n",
    "    # auto_find_batch_size=True, # Find a correct batch size that fits the size of Data.\n",
    "    learning_rate= 5e-5, # Higher learning rate than full fine-tuning.\n",
    "    max_steps = 240,\n",
    "    warmup_steps = 20,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.00,\n",
    "    seed=3047,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    save_steps=20,\n",
    "    # eval_steps=20,\n",
    "    logging_steps=20\n",
    "    # num_train_epochs=5\n",
    ")\n",
    "\n",
    "trainer_instrc = SFTTrainer(\n",
    "    model = model,\n",
    "    args=training_args,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = alpaca_dataset,\n",
    "    dataset_num_proc = 8,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    #packing=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54371569-4cd2-4a63-a85f-ddf272467456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4090. Max memory = 23.41 GB.\n",
      "10.549 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66895c51-7717-4347-8d6b-22a587ba184b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 16:47, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.220600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.894500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.876500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.863700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.817900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.805600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.831600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.819800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.815500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.795500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.849800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.841300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./peft_pre-outputs_llama3_full - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./peft_pre-outputs_llama3_full - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./peft_pre-outputs_llama3_full - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./peft_pre-outputs_llama3_full - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./peft_pre-outputs_llama3_full - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./peft_pre-outputs_llama3_full - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./peft_pre-outputs_llama3_full - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./peft_pre-outputs_llama3_full - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./peft_pre-outputs_llama3_full - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./peft_pre-outputs_llama3_full - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./peft_pre-outputs_llama3_full - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./peft_pre-outputs_llama3_full - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer_instrc_stats = trainer_instrc.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08ac44d3-84b0-440f-b7d8-5f29ea8a3f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012.1207 seconds used for training.\n",
      "16.87 minutes used for training.\n",
      "Peak reserved memory = 15.244 GB.\n",
      "Peak reserved memory for training = 4.695 GB.\n",
      "Peak reserved memory % of max memory = 65.117 %.\n",
      "Peak reserved memory for training % of max memory = 20.056 %.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_instrc_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_instrc_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87ed4118-5ede-4cfb-8654-bd735a17bdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./peft_pre-outputs_llama3_full - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('peft_instrc_unopt/tokenizer_config.json',\n",
       " 'peft_instrc_unopt/special_tokens_map.json',\n",
       " 'peft_instrc_unopt/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_instrc.model.save_pretrained(\"peft_instrc_unopt\")\n",
    "tokenizer.save_pretrained(\"peft_instrc_unopt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c9512e-730d-4031-bca6-7269e9801bbf",
   "metadata": {},
   "source": [
    "# Inference on Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e0baea0-8e4f-4ba5-a67c-d49809f576ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25207a2f-9869-4a16-9255-ad86ef21eab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.path.join(working_dir, \"peft_instrc_unopt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5f083be-1911-4815-bdba-f6268fa1c5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6db78af89740d0a55c7e851cbeee0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "                                        directory,\n",
    "                                        #torch_dtype=torch.bfloat16,\n",
    "                                        #is_trainable=False,\n",
    "                                        #load_in_4bit=True,\n",
    "                                        quantization_config=bnb_config2,\n",
    "                                        device_map = 'cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e75b6cb-a046-4642-b865-f39b0e29cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_hindi_prompt.format(\n",
    "        # \"Describe the planet Earth extensively.\", # instruction\n",
    "        # \"कुछ एक रीसाइक्लिंग अभियान के लिए एक नारा सुझाव दें\",\n",
    "        \"पृथ्वी ग्रह का विस्तृत वर्णन करें\",\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    ),\n",
    "], return_tensors = \"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9eb7b4a1-577b-4924-a481-3dfd3dc3b581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['नीचे एक निर्देश है जो किसी कार्य का वर्णन करता है, जिसे एक इनपुट के साथ जोड़ा गया है जो आगे का संदर्भ प्रदान करता है। ऐसा उत्तर लिखें जो अनुरोध को उचित रूप से पूरा करता हो।\\n\\n### निर्देश:\\nपृथ्वी ग्रह का विस्तृत वर्णन करें\\n\\n### इनपुट:\\n\\n\\n### प्रतिक्रिया:\\nपृथ्वी एक ग्रह है जो सूर्य की कक्षा में सूर्य के करीब है। यह सूर्य से 149.6 मिलियन किलोमीटर दूर है, जो सूर्य की कक्षा का एक तृतीयांश है। पृथ्वी का व्यास 12,742 किलोमीटर है, जो सूर्य से दूरी के त्रुटि के अनुरूप है। पृथ्वी के वार्षिक दो चक्र होते हैं जो कि 12 घंटे के दो चक्र से मिलकर बने होते हैं। पृथ्वी का दो घंटे का एक दिन होता है जो कि सूर्य के पास जाने के लिए एक सूर्य की कक्षा में एक चक्र के लिए लिया जाता है। पृथ्वी का एक साल होता है जो कि पृथ्वी क']\n"
     ]
    }
   ],
   "source": [
    "outputs = loaded_model.generate(**inputs, max_new_tokens = 256, use_cache = True, repetition_penalty = 0.9, temperature = 0.7, early_stopping=False, eos_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c59b30-6341-4223-bd3a-8cc22daf9ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
