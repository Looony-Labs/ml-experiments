{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb319438-0c10-44f6-890b-7a87ce31f238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc, os\n",
    "import peft\n",
    "from peft import LoraConfig, get_peft_model, AutoPeftModelForCausalLM, PeftConfig, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c4a87e-3931-49a1-8379-0ff56f1ab1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "564d0577-408a-49fb-90a0-4d5dc8220700",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 1024\n",
    "dtype = None # None for auto detection\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c52890b-e0bd-46a0-a030-26f1c6f4f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f129402-1358-4e6d-8c16-a62023e9007e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd234a06b4840409d2517cf18692636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"meta-llama/Meta-Llama-3-8B\"\n",
    "device_map = {\"\": 0}\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL, quantization_config=bnb_config, low_cpu_mem_usage=True, device_map=\"auto\", torch_dtype=torch.bfloat16, use_flash_attention_2=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30629cd0-6e11-4502-a858-9b6d0033b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data to train\n",
    "dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.hi\", split = \"train\",)\n",
    "# Use only 10% of data\n",
    "dataset = dataset.train_test_split(train_size = 0.1)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59e3a299-3226-4fc8-8591-172e99d56d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc5717e30f048a7bb181d6f587b109d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:342: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=128,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs\",\n",
    "    #auto_find_batch_size=True, # Find a correct bvatch size that fits the size of Data.\n",
    "    learning_rate= 5e-5, # Higher learning rate than full fine-tuning.\n",
    "    max_steps = 240,\n",
    "    # do_eval=True,\n",
    "    optim = \"adamw_8bit\",\n",
    "    seed=3047,\n",
    "    per_device_train_batch_size=1,\n",
    "    # per_device_eval_batch_size=1,\n",
    "    fp16=True,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    save_steps=20,\n",
    "    # eval_steps=20,\n",
    "    logging_steps=20,\n",
    "    # gradient_accumulation_steps=4,\n",
    "    # num_train_epochs=5\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    args=training_args,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_num_proc = 2,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    packing=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af8cf477-5896-4376-ade9-7d49803db79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4090. Max memory = 23.41 GB.\n",
      "9.344 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96402589-22f7-4f70-81fa-a95fcb9477b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 03:55, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.581600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.582600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.443400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.474600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.500500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.490800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.426500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.467700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.572100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.490200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.476200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b351c286-3a8c-4db5-9fba-1c8091d07413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=240, training_loss=1.5002679506937662, metrics={'train_runtime': 237.1245, 'train_samples_per_second': 1.012, 'train_steps_per_second': 1.012, 'total_flos': 1.156124195684352e+16, 'train_loss': 1.5002679506937662, 'epoch': 0.018393623543838136})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "293258eb-d986-42c8-b6b8-07a08733a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "working_dir = './'\n",
    "\n",
    "output_directory = os.path.join(working_dir, \"peft_pre-outputs_llama3\")\n",
    "# peft_model_path = os.path.join(output_directory, f\"lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cd4086f-97c7-4c15-a79c-6845733998a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./peft_pre-outputs_llama3/tokenizer_config.json',\n",
       " './peft_pre-outputs_llama3/special_tokens_map.json',\n",
       " './peft_pre-outputs_llama3/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(output_directory)\n",
    "tokenizer.save_pretrained(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5358e04b-c8f6-43d7-b031-572e6de41a90",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e40178c-6bee-4895-91ca-f883d9efa6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config2 = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ada0eba-53f7-4813-860c-6b2dfd0109c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90433152a882466c8b04661613188597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_with_adapters_model = AutoPeftModelForCausalLM.from_pretrained(output_directory)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_directory)\n",
    "\n",
    "# loaded_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#                                         output_directory,\n",
    "#                                         torch_dtype=torch.bfloat16,\n",
    "#                                         #load_in_4bit=True,\n",
    "#                                         quantization_config=bnb_config2,\n",
    "#                                         device_map = 'cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "447ab802-1306-4cac-bc68-6beed82c8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with base model\n",
    "model = base_with_adapters_model.merge_and_unload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "463114ad-3420-440d-85fa-5409be7d087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.path.join(working_dir, \"peft_pre-outputs_llama3_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f1dfde-8ea3-42a4-999a-dd01b1e6ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(dir)\n",
    "tokenizer.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5207e72a-0d9f-4108-9918-f504f1bd5b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function returns the outputs from the model received, and inputs.\n",
    "def get_outputs(model, inputs, max_new_tokens=100):\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        repetition_penalty=1.5, #Avoid repetition.\n",
    "        early_stopping=False, #The model can stop before reach the max_length\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9011bea9-b62a-428b-a794-ae0a75320886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be2a717b759467eacb3d7191499058a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load the merged model\n",
    "device_map = {\"\": 0}\n",
    "model = AutoModelForCausalLM.from_pretrained(dir, torch_dtype=dtype, quantization_config=bnb_config2, device_map=device_map)\n",
    "tokenizer = AutoTokenizer.from_pretrained(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1186c406-4469-45c8-8093-4a34e303d1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['फाइबोनैचि अनुक्रम जारी रखें: 1, 1, 2, 3, 5, 8,... और इस तरह। फ़िरॉनाचिस संख्या के लिए एक सरल परिभाषित करो:\\nfibs(n) = fibs( n - 4 ) +fib (n-3)\\nप्रयोग करनेवालों में, यह भूलना, चिंताजनक है । बेशक, हमेशा\\nएक प्राथमिक रूप निर्दिष्ट करते, यद्यपि, यथासंभव आसान\\nहै, ताकि आप इसके सम्बन्ध कुछ अंश देख सकें।\\n\\nउदाहरण:\\n\\n``']\n"
     ]
    }
   ],
   "source": [
    "input_sentences = tokenizer(\"फाइबोनैचि अनुक्रम जारी रखें: 1, 1, 2, 3, 5, 8,\", return_tensors=\"pt\").to('cuda')\n",
    "foundational_outputs_sentence = get_outputs(model, input_sentences, max_new_tokens=128)\n",
    "\n",
    "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9d6b76-06b9-4d2b-9427-0d23d9f23ecd",
   "metadata": {},
   "source": [
    "# FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae531428-1161-4838-99e2-58667f142415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "alpaca_dataset = load_dataset(\"FreedomIntelligence/alpaca-gpt4-hindi\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "617b55cb-cc74-4b96-a945-22093174dfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversations': [{'from': 'human', 'value': 'कुछ एक रीसाइक्लिंग अभियान के लिए एक नारा सुझाव दें।\\n'}, {'from': 'gpt', 'value': '1. \"ग्रीन भविष्य के लिए एक साथ: कम करें, पुन: उपयोग करें, रीसाइकल करें।\"\\n2. \"एक बेहतर कल के लिए आज ही रीसाइकल करें।\"\\n3. \"अपने कचरे को खजाना बनाएं - रीसाइकल करें!\"\\n4. \"जीवन के चक्र के लिए रीसाइकल करें।\"\\n5. \"संसाधन बचाएं, अधिक रीसाइकल करें।\"'}], 'id': '23712'}\n"
     ]
    }
   ],
   "source": [
    "print(alpaca_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24973be4-8851-4af3-a5c9-d97b4dcf2c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_hindi_prompt=\"\"\"नीचे एक निर्देश है जो किसी कार्य का वर्णन करता है, जिसे एक इनपुट के साथ जोड़ा गया है जो आगे का संदर्भ प्रदान करता है। ऐसा उत्तर लिखें जो अनुरोध को उचित रूप से पूरा करता हो।\n",
    "\n",
    "### निर्देश:\n",
    "{}\n",
    "\n",
    "### इनपुट:\n",
    "{}\n",
    "\n",
    "### प्रतिक्रिया:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e392dbf-4d85-452b-afbf-e67e4648dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def formatting_prompts_func(conversations):\n",
    "    texts = []\n",
    "    conversations = conversations[\"conversations\"]\n",
    "    for convo in conversations:\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        # Careful Aya Dataset does not have an input!\n",
    "        text = alpaca_hindi_prompt.format(convo[0][\"value\"], \"\", convo[1][\"value\"]) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "alpaca_dataset = alpaca_dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d13ad3ae-872d-48bd-a38d-fa845944091b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c624b933554a1e93f859eeb1e24bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/49969 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=128,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# base_model_with_new_adapter = get_peft_model(model, peft_config)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs\",\n",
    "    auto_find_batch_size=True, # Find a correct batch size that fits the size of Data.\n",
    "    learning_rate= 5e-5, # Higher learning rate than full fine-tuning.\n",
    "    max_steps = 240,\n",
    "    optim = \"adamw_8bit\",\n",
    "    seed=3047,\n",
    "    fp16=True,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    save_steps=20,\n",
    "    # eval_steps=20,\n",
    "    logging_steps=20\n",
    "    # num_train_epochs=5\n",
    ")\n",
    "\n",
    "trainer_instrc = SFTTrainer(\n",
    "    model = model,\n",
    "    args=training_args,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = alpaca_dataset,\n",
    "    dataset_num_proc = 2,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    #packing=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54371569-4cd2-4a63-a85f-ddf272467456",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66895c51-7717-4347-8d6b-22a587ba184b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 09:43, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.058900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.864400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.886300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.834900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.872100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.847400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.887000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.827100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.833400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.829900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.803600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./peft_pre-outputs_llama3_full - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./peft_pre-outputs_llama3_full - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./peft_pre-outputs_llama3_full - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./peft_pre-outputs_llama3_full - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./peft_pre-outputs_llama3_full - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./peft_pre-outputs_llama3_full - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./peft_pre-outputs_llama3_full - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./peft_pre-outputs_llama3_full - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./peft_pre-outputs_llama3_full - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./peft_pre-outputs_llama3_full - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./peft_pre-outputs_llama3_full - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./peft_pre-outputs_llama3_full - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer_instrc_stats = trainer_instrc.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08ac44d3-84b0-440f-b7d8-5f29ea8a3f5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'start_gpu_memory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m used_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmax_memory_reserved() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m used_memory_for_lora \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(used_memory \u001b[38;5;241m-\u001b[39m \u001b[43mstart_gpu_memory\u001b[49m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      3\u001b[0m used_percentage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(used_memory         \u001b[38;5;241m/\u001b[39mmax_memory\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      4\u001b[0m lora_percentage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(used_memory_for_lora\u001b[38;5;241m/\u001b[39mmax_memory\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'start_gpu_memory' is not defined"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87ed4118-5ede-4cfb-8654-bd735a17bdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ydatta/anaconda3/envs/llm_exp_lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./peft_pre-outputs_llama3_full - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('peft_instrc_unopt/tokenizer_config.json',\n",
       " 'peft_instrc_unopt/special_tokens_map.json',\n",
       " 'peft_instrc_unopt/tokenizer.json')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_instrc.model.save_pretrained(\"peft_instrc_unopt\")\n",
    "tokenizer.save_pretrained(\"peft_instrc_unopt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c9512e-730d-4031-bca6-7269e9801bbf",
   "metadata": {},
   "source": [
    "# Inference on Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25207a2f-9869-4a16-9255-ad86ef21eab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.path.join(working_dir, \"peft_instrc_unopt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5f083be-1911-4815-bdba-f6268fa1c5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b84acb935024ff984244fc4702adf2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "                                        directory,\n",
    "                                        #torch_dtype=torch.bfloat16,\n",
    "                                        #is_trainable=False,\n",
    "                                        #load_in_4bit=True,\n",
    "                                        quantization_config=bnb_config2,\n",
    "                                        device_map = 'cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e75b6cb-a046-4642-b865-f39b0e29cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_hindi_prompt.format(\n",
    "        # \"Describe the planet Earth extensively.\", # instruction\n",
    "        # \"कुछ एक रीसाइक्लिंग अभियान के लिए एक नारा सुझाव दें\",\n",
    "        \"पृथ्वी ग्रह का विस्तृत वर्णन करें\",\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    ),\n",
    "], return_tensors = \"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9eb7b4a1-577b-4924-a481-3dfd3dc3b581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['नीचे एक निर्देश है जो किसी कार्य का वर्णन करता है, जिसे एक इनपुट के साथ जोड़ा गया है जो आगे का संदर्भ प्रदान करता है। ऐसा उत्तर लिखें जो अनुरोध को उचित रूप से पूरा करता हो।\\n\\n### निर्देश:\\nपृथ्वी ग्रह का विस्तृत वर्णन करें\\n\\n### इनपुट:\\n\\n\\n### प्रतिक्रिया:\\nएक अस्थायी, परिवर्तनशील और बहुत अध्ययन योग्यतापूण, हम सभीज्ञात चंद्रमा, मंगलग्लोबल तंतुओं (जैसेकि भूमि, जल एवम् एअर) दिए गए हर वस्तविकतामय आकाशीय बिंदु। यह सबसेबहतरक्षततम धर्मों-विशेषकर शीतोष्ण कटिबंधी-सब्जाओंकासंस्करण करनेवालेरेगियन्धानमुक्तकृषिविहारकौच्चागुप्तमध्यक्षणभूतधरादुनिन्नगरिमाधानकेन्मकड़क्रस्वर्गजन्मन्हीं बनाए रखनेलोकाभिगमनसमुच्छेदसन्दर्शिभावएंवपरिणामकोटिप्रयोजना-इन सब तक पहुँचनेसंभवतः उन महानताईँओनलेकर सक्षकुण्डली। \\n\\nइस टेप्स्टीरियलिज़्मकुंटोनेटीविधिसमूहमेजूदागराज्यमोहारताकैरवराहीनखिलफसलेभोजांउष्ठाबुधाननिदूषीकोर्णहर']\n"
     ]
    }
   ],
   "source": [
    "# input_sentences = tokenizer(\"फाइबोनैचि अनुक्रम जारी रखें: 1, 1, 2, 3, 5, 8,\", return_tensors=\"pt\").to('cuda')\n",
    "# foundational_outputs_sentence = get_outputs(loaded_model, input_sentences, max_new_tokens=50)\n",
    "\n",
    "outputs = loaded_model.generate(**inputs, max_new_tokens = 256, use_cache = True, repetition_penalty = 1.5, temperature = 0.7, early_stopping=False, eos_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c59b30-6341-4223-bd3a-8cc22daf9ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
