{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43cd0840-68f0-413f-b1fc-b090e1b367aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch, gc, os\n",
    "from datasets import load_dataset\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f699a56-93cb-45e1-b8bc-47004c9f3c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048\n",
    "dtype = None # None for auto detection\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b17af6d-0782-473b-9c7d-f3781aa09042",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d634d666-2254-4acb-9a67-07c75194c255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.5\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.41 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f097679-7bef-4d36-8559-5fc241379016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading input_embeddings to disk to save VRAM\n",
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Casting embed_tokens to float32\n",
      "Unsloth: Casting lm_head to float32\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\", \n",
    "                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,   # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1761690-9e15-4ca0-8fe2-805badabb834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data to train\n",
    "dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.hi\", split = \"train\",)\n",
    "# Use only 10% of data\n",
    "dataset = dataset.train_test_split(train_size = 0.1)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c72c8b89-3565-4207-9ab6-ee5b11592071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = UnslothTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing=False,\n",
    "\n",
    "    args = UnslothTrainingArguments(\n",
    "        per_device_train_batch_size = 1,\n",
    "        # gradient_accumulation_steps = 8,\n",
    "\n",
    "        # Use warmup_ratio and num_train_epochs for longer runs!\n",
    "        max_steps = 240,\n",
    "        warmup_steps = 20,\n",
    "        # warmup_ratio = 0.1,\n",
    "        # num_train_epochs = 1,\n",
    "\n",
    "        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n",
    "        learning_rate = 5e-5,\n",
    "        embedding_learning_rate = 1e-5,\n",
    "\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 20,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        # lr_scheduler_type = \"cosine\",\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efb83c0b-e779-4386-abef-c5dcefa527d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4090. Max memory = 23.41 GB.\n",
      "9.648 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b1f4086-62b8-4329-ad27-465f9f9e28db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 16,309 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 1 | Total steps = 240\n",
      " \"-____-\"     Number of trainable parameters = 1,386,217,472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Setting lr = 1.00e-05 instead of 5.00e-05 for embed_tokens.\n",
      "Unsloth: Setting lr = 1.00e-05 instead of 5.00e-05 for lm_head.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 01:49, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.512600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.183400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.255400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.351000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.313800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.311000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.359000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.164900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.109300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.150700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.118500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.919700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "186dbcf8-5637-4176-b97f-9c506555b355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=240, training_loss=1.229105544090271, metrics={'train_runtime': 110.633, 'train_samples_per_second': 2.169, 'train_steps_per_second': 2.169, 'total_flos': 5778830780596224.0, 'train_loss': 1.229105544090271, 'epoch': 0.014715801091421914})\n"
     ]
    }
   ],
   "source": [
    "print(trainer_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13331f1d-7164-4715-b1c8-68bbe878f339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110.633 seconds used for training.\n",
      "1.84 minutes used for training.\n",
      "Peak reserved memory = 17.723 GB.\n",
      "Peak reserved memory for training = 8.075 GB.\n",
      "Peak reserved memory % of max memory = 75.707 %.\n",
      "Peak reserved memory for training % of max memory = 34.494 %.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b6d52be-15e3-448a-8d2b-d54dac960339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 78.9 out of 125.5 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            | 11/32 [00:00<00:00, 53.13it/s]We will save to Disk and not RAM now.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:06<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('lora_unsloth_opt_full/tokenizer_config.json',\n",
       " 'lora_unsloth_opt_full/special_tokens_map.json',\n",
       " 'lora_unsloth_opt_full/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained_merged(\"lora_unsloth_opt_full\", tokenizer, save_method=\"merged_16bit\")\n",
    "tokenizer.save_pretrained(\"lora_unsloth_opt_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffffa837-0943-43c5-b6bf-fad42f73facd",
   "metadata": {},
   "source": [
    "# Instruction Finetuning\n",
    "We now use alpaca dataset in hindi for instruction fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f388412-94b4-479c-bc41-7288b0bcfa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55d235ea-493d-4243-8922-4ba779de3a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "alpaca_dataset = load_dataset(\"FreedomIntelligence/alpaca-gpt4-hindi\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e73197-90e3-4452-87ee-aeed70d9a50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversations': [{'from': 'human', 'value': '‡§ï‡•Å‡§õ ‡§è‡§ï ‡§∞‡•Ä‡§∏‡§æ‡§á‡§ï‡•ç‡§≤‡§ø‡§Ç‡§ó ‡§Ö‡§≠‡§ø‡§Ø‡§æ‡§® ‡§ï‡•á ‡§≤‡§ø‡§è ‡§è‡§ï ‡§®‡§æ‡§∞‡§æ ‡§∏‡•Å‡§ù‡§æ‡§µ ‡§¶‡•á‡§Ç‡•§\\n'}, {'from': 'gpt', 'value': '1. \"‡§ó‡•ç‡§∞‡•Ä‡§® ‡§≠‡§µ‡§ø‡§∑‡•ç‡§Ø ‡§ï‡•á ‡§≤‡§ø‡§è ‡§è‡§ï ‡§∏‡§æ‡§•: ‡§ï‡§Æ ‡§ï‡§∞‡•á‡§Ç, ‡§™‡•Å‡§®: ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç, ‡§∞‡•Ä‡§∏‡§æ‡§á‡§ï‡§≤ ‡§ï‡§∞‡•á‡§Ç‡•§\"\\n2. \"‡§è‡§ï ‡§¨‡•á‡§π‡§§‡§∞ ‡§ï‡§≤ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§ú ‡§π‡•Ä ‡§∞‡•Ä‡§∏‡§æ‡§á‡§ï‡§≤ ‡§ï‡§∞‡•á‡§Ç‡•§\"\\n3. \"‡§Ö‡§™‡§®‡•á ‡§ï‡§ö‡§∞‡•á ‡§ï‡•ã ‡§ñ‡§ú‡§æ‡§®‡§æ ‡§¨‡§®‡§æ‡§è‡§Ç - ‡§∞‡•Ä‡§∏‡§æ‡§á‡§ï‡§≤ ‡§ï‡§∞‡•á‡§Ç!\"\\n4. \"‡§ú‡•Ä‡§µ‡§® ‡§ï‡•á ‡§ö‡§ï‡•ç‡§∞ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∞‡•Ä‡§∏‡§æ‡§á‡§ï‡§≤ ‡§ï‡§∞‡•á‡§Ç‡•§\"\\n5. \"‡§∏‡§Ç‡§∏‡§æ‡§ß‡§® ‡§¨‡§ö‡§æ‡§è‡§Ç, ‡§Ö‡§ß‡§ø‡§ï ‡§∞‡•Ä‡§∏‡§æ‡§á‡§ï‡§≤ ‡§ï‡§∞‡•á‡§Ç‡•§\"'}], 'id': '23712'}\n"
     ]
    }
   ],
   "source": [
    "print(alpaca_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "236c2a5b-6646-4073-b29f-11172ba6bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_hindi_prompt=\"\"\"‡§®‡•Ä‡§ö‡•á ‡§è‡§ï ‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂ ‡§π‡•à ‡§ú‡•ã ‡§ï‡§ø‡§∏‡•Ä ‡§ï‡§æ‡§∞‡•ç‡§Ø ‡§ï‡§æ ‡§µ‡§∞‡•ç‡§£‡§® ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, ‡§ú‡§ø‡§∏‡•á ‡§è‡§ï ‡§á‡§®‡§™‡•Å‡§ü ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ú‡•ã‡§°‡§º‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à ‡§ú‡•ã ‡§Ü‡§ó‡•á ‡§ï‡§æ ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ ‡§ê‡§∏‡§æ ‡§â‡§§‡•ç‡§§‡§∞ ‡§≤‡§ø‡§ñ‡•á‡§Ç ‡§ú‡•ã ‡§Ö‡§®‡•Å‡§∞‡•ã‡§ß ‡§ï‡•ã ‡§â‡§ö‡§ø‡§§ ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§™‡•Ç‡§∞‡§æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•ã‡•§\n",
    "\n",
    "### ‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂:\n",
    "{}\n",
    "\n",
    "### ‡§á‡§®‡§™‡•Å‡§ü:\n",
    "{}\n",
    "\n",
    "### ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d73e8b7b-e3a9-4314-a027-1546e1cf1d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def formatting_prompts_func(conversations):\n",
    "    texts = []\n",
    "    conversations = conversations[\"conversations\"]\n",
    "    for convo in conversations:\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        # Careful Aya Dataset does not have an input!\n",
    "        text = alpaca_hindi_prompt.format(convo[0][\"value\"], \"\", convo[1][\"value\"]) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "alpaca_dataset = alpaca_dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c781e911-1466-44b6-b2be-571e33912129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.5\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.41 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acbeccc0a577405bbc2351bb1609bba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"lora_unsloth_opt_full\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de111e3e-a112-4c90-86f6-c53572101dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading input_embeddings to disk to save VRAM\n",
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Casting embed_tokens to float32\n",
      "Unsloth: Casting lm_head to float32\n"
     ]
    }
   ],
   "source": [
    "# Use Lora adapters to update only 1 - 10 % of params\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "                      \"embed_tokens\", \"lm_head\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,   # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6294670-49d4-44dc-b0e3-15cfedc5f938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,386,217,472 || all params: 9,416,478,720 || trainable%: 14.7212\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52e23166-09e4-44f6-bfa3-af93915bb341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer_instrc = UnslothTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = alpaca_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 8,\n",
    "\n",
    "    args = UnslothTrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "\n",
    "        # Use num_train_epochs and warmup_ratio for longer runs!\n",
    "        max_steps = 240,\n",
    "        warmup_steps = 20,\n",
    "        # warmup_ratio = 0.1,\n",
    "        # num_train_epochs = 1,\n",
    "\n",
    "        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n",
    "        learning_rate = 5e-5,\n",
    "        embedding_learning_rate = 1e-5,\n",
    "\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 20,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.00,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs_instrc\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21764e4b-18f2-46ba-a89b-49a8fdfd82ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4090. Max memory = 23.41 GB.\n",
      "9.668 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62c17186-9075-4e11-96e9-109138bba006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 49,969 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 240\n",
      " \"-____-\"     Number of trainable parameters = 1,386,217,472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Setting lr = 1.00e-05 instead of 5.00e-05 for embed_tokens.\n",
      "Unsloth: Setting lr = 1.00e-05 instead of 5.00e-05 for lm_head.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 11:09, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.063800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.868400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.778000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.854500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.815900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.786600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.839100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.761900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.808600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.755600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.791200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.768600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_instrc_stats = trainer_instrc.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f11e6b4-52dd-49de-90c0-c1e6dbf03fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=240, training_loss=0.8241770381728808, metrics={'train_runtime': 665.8177, 'train_samples_per_second': 2.884, 'train_steps_per_second': 0.36, 'total_flos': 6.155502849456538e+16, 'train_loss': 0.8241770381728808, 'epoch': 0.03842305383229938})\n"
     ]
    }
   ],
   "source": [
    "print(trainer_instrc_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14c0d242-7bc6-493a-94a0-e6834db237c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "672.1318 seconds used for training.\n",
      "11.2 minutes used for training.\n",
      "Peak reserved memory = 21.492 GB.\n",
      "Peak reserved memory for training = 11.824 GB.\n",
      "Peak reserved memory % of max memory = 91.807 %.\n",
      "Peak reserved memory for training % of max memory = 50.508 %.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_instrc_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_instrc_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "733edba3-b76e-40cf-a43f-b5b3d039d512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_unsloth_instrc_opt/tokenizer_config.json',\n",
       " 'lora_unsloth_instrc_opt/special_tokens_map.json',\n",
       " 'lora_unsloth_instrc_opt/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save final model\n",
    "model.save_pretrained(\"lora_unsloth_instrc_opt\")\n",
    "tokenizer.save_pretrained(\"lora_unsloth_instrc_opt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6f04572-072d-4e2f-9324-336feef1a1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: You are pushing to hub, but you passed your HF username = saucam.\n",
      "We shall truncate saucam/lora-llama-3-8B-unsloth-opt to lora-llama-3-8B-unsloth-opt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 77.62 out of 125.5 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 6/32 [00:00<00:00, 52.57it/s]We will save to Disk and not RAM now.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:06<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b2d65c922440328bff06fdcfdeae5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/561 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eec4d3301ab4a8c8108bea26a8645ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283a37e0334d4efa86a64d30c51faa42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332213ea8a5340fb9001a7247c993471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107df8401ff44e09be6dfe3b0ac0593a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6a5ea4cf12463e9d6fe3566b38cfe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Saved merged model to https://huggingface.co/saucam/lora-llama-3-8B-unsloth-opt\n"
     ]
    }
   ],
   "source": [
    "# model.push_to_hub_merged(\"saucam/lora-llama-3-8B-unsloth-opt\", tokenizer, save_method = \"merged_16bit\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff00e9a2-7dee-46e7-b494-3e1d7b26aec7",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "348baa43-cce5-487b-8efa-25acfb2d2aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b3eda4-041e-4b26-9e5b-8e7673b8954c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.5\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.41 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4c808aab46497d9e2cfc4b655bc602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"lora_unsloth_instrc_opt\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# alpaca_prompt = You MUST copy from above!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c8ef14a-576f-4350-898c-5a783f180100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>‡§®‡•Ä‡§ö‡•á ‡§è‡§ï ‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂ ‡§π‡•à ‡§ú‡•ã ‡§ï‡§ø‡§∏‡•Ä ‡§ï‡§æ‡§∞‡•ç‡§Ø ‡§ï‡§æ ‡§µ‡§∞‡•ç‡§£‡§® ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, ‡§ú‡§ø‡§∏‡•á ‡§è‡§ï ‡§á‡§®‡§™‡•Å‡§ü ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ú‡•ã‡§°‡§º‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à ‡§ú‡•ã ‡§Ü‡§ó‡•á ‡§ï‡§æ ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ ‡§ê‡§∏‡§æ ‡§â‡§§‡•ç‡§§‡§∞ ‡§≤‡§ø‡§ñ‡•á‡§Ç ‡§ú‡•ã ‡§Ö‡§®‡•Å‡§∞‡•ã‡§ß ‡§ï‡•ã ‡§â‡§ö‡§ø‡§§ ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§™‡•Ç‡§∞‡§æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•ã‡•§\\n\\n### ‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂:\\n‡§™‡•É‡§•‡•ç‡§µ‡•Ä ‡§ó‡•ç‡§∞‡§π ‡§ï‡§æ ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§µ‡§∞‡•ç‡§£‡§® ‡§ï‡§∞‡•á‡§Ç‡•§\\n\\n### ‡§á‡§®‡§™‡•Å‡§ü:\\n\\n\\n### ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ:\\n‡§™‡•É‡§•‡•ç‡§µ‡•Ä ‡§è‡§ï ‡§ó‡•ç‡§∞‡§π ‡§π‡•à ‡§ú‡•ã ‡§∏‡•Ç‡§∞‡•ç‡§Ø ‡§ï‡•á ‡§ö‡§æ‡§∞‡•ã‡§Ç ‡§ì‡§∞ ‡§è‡§ï ‡§∏‡•Ç‡§∞‡•ç‡§Ø ‡§ï‡•á ‡§ö‡§æ‡§∞‡•ã‡§Ç ‡§ì‡§∞ ‡§ò‡•Ç‡§Æ‡§§‡§æ ‡§π‡•à, ‡§ú‡•ã ‡§∏‡•Ç‡§∞‡•ç‡§Ø ‡§ï‡•á ‡§∏‡§æ‡§• ‡§∏‡§æ‡§• ‡§è‡§ï ‡§∏‡•Ç‡§∞‡•ç‡§Ø ‡§ï‡•á ‡§ö‡§æ‡§∞‡•ã‡§Ç ‡§ì‡§∞ ‡§ò‡•Ç‡§Æ‡§§‡§æ ‡§π‡•à‡•§ ‡§™‡•É‡§•‡•ç‡§µ‡•Ä ‡§∏‡•Ç‡§∞‡•ç‡§Ø ‡§ï‡•á ‡§ö‡§æ‡§∞‡•ã‡§Ç ‡§ì‡§∞ ‡§è‡§ï ‡§∏‡•Ç‡§∞‡•ç‡§Ø ‡§ï‡•á ‡§ö‡§æ‡§∞‡•ã‡§Ç ‡§ì‡§∞ ‡§ò‡•Ç‡§Æ‡§§‡§æ ‡§π‡•à, ‡§ú‡•ã ‡§∏‡•Ç‡§∞‡•ç‡§Ø ‡§ï‡•á ‡§∏‡§æ‡§• ‡§∏‡§æ‡§• ‡§è‡§ï ‡§∏‡•Ç‡§∞‡•ç‡§Ø ‡§ï‡•á ‡§ö‡§æ‡§∞‡•ã‡§Ç ‡§ì‡§∞ ‡§ò‡•Ç‡§Æ‡§§‡§æ ‡§π‡•à‡•§ ‡§™‡•É‡§•‡•ç‡§µ‡•Ä ‡§ï‡§æ ‡§µ‡§ø‡§∏‡•ç‡§§‡§æ‡§∞ 7,917,917 ‡§Æ‡•Ä‡§≤ ‡§π‡•à ‡§î‡§∞ ‡§Ø‡§π ‡§∏‡•Ç‡§∞‡•ç‡§Ø ‡§ï‡•á ‡§∏‡§æ‡§• ‡§∏‡§æ‡§• ‡§è‡§ï ‡§∏‡•Ç‡§∞‡•ç‡§Ø ‡§ï‡•á ‡§ö‡§æ‡§∞‡•ã‡§Ç ‡§ì‡§∞ ‡§ò‡•Ç‡§Æ‡§§‡§æ ‡§π‡•à‡•§ ‡§™‡•É‡§•‡•ç‡§µ‡•Ä ‡§ï‡•Ä ‡§∏‡§§‡§π ‡§ï‡§æ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡§´‡§≤ 196,940,800 ‡§µ‡§∞‡•ç‡§ó ‡§Æ‡•Ä‡§≤ ‡§π‡•à ‡§î‡§∞ ‡§Ø‡§π ‡§∏‡•Ç‡§∞‡•ç‡§Ø ‡§ï‡•á ‡§∏‡§æ‡§• ‡§∏‡§æ‡§• ‡§è‡§ï ‡§∏‡•Ç‡§∞‡•ç‡§Ø ‡§ï‡•á ‡§ö‡§æ‡§∞‡•ã‡§Ç ‡§ì‡§∞ ‡§ò‡•Ç‡§Æ‡§§‡§æ ‡§π‡•à‡•§ ‡§™‡•É‡§•‡•ç‡§µ‡•Ä ‡§ï‡•Ä ‡§∏‡§§‡§π ‡§ï‡§æ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡§´‡§≤ 196,940,800 ‡§µ‡§∞‡•ç‡§ó ‡§Æ‡•Ä‡§≤ ‡§π‡•à ‡§î‡§∞ ‡§Ø‡§π']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_hindi_prompt.format(\n",
    "        # \"Describe the planet Earth extensively.\", # instruction\n",
    "        \"‡§™‡•É‡§•‡•ç‡§µ‡•Ä ‡§ó‡•ç‡§∞‡§π ‡§ï‡§æ ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§µ‡§∞‡•ç‡§£‡§® ‡§ï‡§∞‡•á‡§Ç‡•§\",\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    ),\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True, repetition_penalty = 0.9, temperature = 0.5)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc8d85b-acb3-4613-84c2-69cdaf0be335",
   "metadata": {},
   "source": [
    "# Inference using transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba222cdb-7f0e-46ac-b6da-18fa9ca7d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00de90f2-99dd-4d05-bed9-9684806f9085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaafbbf3d68545ae8f3020c0bff677fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        \"lora_unsloth_instrc_opt\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        load_in_4bit = load_in_4bit,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lora_unsloth_instrc_opt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28a21e5e-ccbb-415a-8447-b9edc19a041f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>‡§®‡•Ä‡§ö‡•á ‡§è‡§ï ‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂ ‡§π‡•à ‡§ú‡•ã ‡§ï‡§ø‡§∏‡•Ä ‡§ï‡§æ‡§∞‡•ç‡§Ø ‡§ï‡§æ ‡§µ‡§∞‡•ç‡§£‡§® ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, ‡§ú‡§ø‡§∏‡•á ‡§è‡§ï ‡§á‡§®‡§™‡•Å‡§ü ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ú‡•ã‡§°‡§º‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à ‡§ú‡•ã ‡§Ü‡§ó‡•á ‡§ï‡§æ ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ ‡§ê‡§∏‡§æ ‡§â‡§§‡•ç‡§§‡§∞ ‡§≤‡§ø‡§ñ‡•á‡§Ç ‡§ú‡•ã ‡§Ö‡§®‡•Å‡§∞‡•ã‡§ß ‡§ï‡•ã ‡§â‡§ö‡§ø‡§§ ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§™‡•Ç‡§∞‡§æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•ã‡•§\\n\\n### ‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂:\\n‡§™‡•É‡§•‡•ç‡§µ‡•Ä ‡§ó‡•ç‡§∞‡§π ‡§ï‡§æ ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§µ‡§∞‡•ç‡§£‡§® ‡§ï‡§∞‡•á‡§Ç‡•§\\n\\n### ‡§á‡§®‡§™‡•Å‡§ü:\\n\\n\\n### ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ:\\n‡§™‡•É‡§•‡•ç‡§µ‡•Ä ‡§è‡§ï ‡§ó‡•ç‡§∞‡§π ‡§π‡•à ‡§ú‡•ã ‡§∏‡•Ç‡§∞‡•ç‡§Ø ‡§ï‡•á ‡§è‡§ï ‡§∞‡•â‡§ï‡•á‡§ü ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§∏‡•Ç‡§∞‡•ç‡§Ø ‡§ï‡•á ‡§≤‡•ã‡§ó ‡§ï‡•á ‡§™‡•É‡§•‡•ç‡§µ‡•Ä ‡§ó‡•ç‡§∞‡§π ‡§∏‡§ø‡§∏‡•ç‡§ü‡§Æ ‡§ï‡§æ ‡§è‡§ï ‡§π‡§ø‡§∏‡•ç‡§∏‡§æ ‡§π‡•à‡•§ ‡§™‡•É‡§•‡•ç‡§µ‡•Ä ‡§è‡§ï ‡§ó‡•ç‡§∞‡§π ‡§ï‡•Ä ‡§§‡§æ‡§∞‡•á ‡§ï‡•á ‡§∏‡•å‡§∞ ‡§Æ‡§Ç‡§°‡§≤ ‡§Æ‡•á‡§Ç ‡§∏‡•á ‡§§‡•Ä‡§∏‡§∞‡§æ ‡§∏‡§¨‡§∏‡•á ‡§¨‡§°‡§º‡§æ ‡§ó‡•ç‡§∞‡§π ‡§π‡•à ‡§ú‡•ã ‡§∏‡•å‡§∞ ‡§Æ‡§Ç‡§°‡§≤ ‡§ï‡•á ‡§ó‡•ç‡§∞‡§π‡•ã‡§Ç ‡§ï‡•Ä ‡§π‡§ø‡§∏‡•ç‡§∏‡§æ ‡§π‡•à‡•§ ‡§ó‡•ç‡§∞‡§π ‡§ï‡•Ä ‡§§‡§æ‡§∞‡§æ ‡§∏‡•å‡§∞ ‡§Æ‡§Ç‡§°‡§≤ ‡§Æ‡•á‡§Ç ‡§∏‡§¨‡§∏‡•á ‡§ú‡•ç‡§Ø‡§æ‡§¶‡§æ ‡§Æ']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_hindi_prompt.format(\n",
    "        # \"Describe the planet Earth extensively.\", # instruction\n",
    "        \"‡§™‡•É‡§•‡•ç‡§µ‡•Ä ‡§ó‡•ç‡§∞‡§π ‡§ï‡§æ ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§µ‡§∞‡•ç‡§£‡§® ‡§ï‡§∞‡•á‡§Ç‡•§\",\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    ),\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True, repetition_penalty = 0.9, temperature = 0.9)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54b7238-8690-4d16-90fd-75c16edd76f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
