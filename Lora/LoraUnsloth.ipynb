{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9688bf5c-12cd-497e-8495-cc5c841e4907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch, gc, os\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f82a0384-b24b-4a61-9bef-e90c6e09cbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048\n",
    "dtype = None # None for auto detection\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "789f271c-0d17-4abe-8962-689a430613a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13e4f5c5-59b1-4225-90b7-ca04919eb714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.5\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.41 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# We will use TinyLlama for this demo, but you can choose any other model\n",
    "MODEL = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ee88c6b-afcf-4852-a762-49a61ddab646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# Use Lora adapters to update only 1 - 10 % of params\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,   # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb4b6626-f0f9-40de-a885-da5624476715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data to train\n",
    "dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.hi\", split = \"train\",)\n",
    "# Use only 10% of data\n",
    "dataset = dataset.train_test_split(train_size = 0.1)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30323a80-69fa-4345-9e3b-540b242fdb10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'text'],\n",
       "    num_rows: 16309\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a969e74-f3bc-4c0d-899c-41d12353b237",
   "metadata": {},
   "source": [
    "# Continued Pre-training\n",
    "\n",
    "This is continued pre-training to instill more knowledge into already trained base model. We just need a dataset with text field containing data we need to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47d52c20-d34a-48cf-bfce-fc8f89e8aa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing=False,\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 1,\n",
    "        # gradient_accumulation_steps = 8,\n",
    "\n",
    "        # Use warmup_ratio and num_train_epochs for longer runs!\n",
    "        max_steps = 240,\n",
    "        warmup_steps = 20,\n",
    "        # warmup_ratio = 0.1,\n",
    "        # num_train_epochs = 1,\n",
    "\n",
    "        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n",
    "        learning_rate = 5e-5,\n",
    "        # embedding_learning_rate = 1e-5,\n",
    "\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 20,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        # lr_scheduler_type = \"cosine\",\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af0b8d23-c874-4df5-a78a-4d2aa261faca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4090. Max memory = 23.41 GB.\n",
      "6.699 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a99461bc-4de4-4c55-b2b0-c9d8bf4568cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 16,309 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 1 | Total steps = 240\n",
      " \"-____-\"     Number of trainable parameters = 335,544,320\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 01:23, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.236500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.272200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.373800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.426200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.318800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.425500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.196600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.142300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.158400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.139100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.939900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39f64e28-089f-4371-bfc6-665909cbd3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.8644 seconds used for training.\n",
      "1.41 minutes used for training.\n",
      "Peak reserved memory = 9.645 GB.\n",
      "Peak reserved memory for training = 2.946 GB.\n",
      "Peak reserved memory % of max memory = 41.2 %.\n",
      "Peak reserved memory for training % of max memory = 12.584 %.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bddd2a2b-77e7-42aa-bbc5-ba46f9c88225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 81.49 out of 125.5 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 24/32 [00:00<00:00, 56.83it/s]We will save to Disk and not RAM now.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:02<00:00, 11.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('lora_unsloth_unopt_full/tokenizer_config.json',\n",
       " 'lora_unsloth_unopt_full/special_tokens_map.json',\n",
       " 'lora_unsloth_unopt_full/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained_merged(\"lora_unsloth_unopt_full\", tokenizer, save_method=\"merged_16bit\")\n",
    "tokenizer.save_pretrained(\"lora_unsloth_unopt_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00884118-550f-445e-90cd-26532f4f4c78",
   "metadata": {},
   "source": [
    "# Instruction Finetuning\n",
    "\n",
    "We now use alpaca dataset in hindi for instruction fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f764cfee-9d7f-4b9b-b237-1cfa1a9b0f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ca45ae7-6eab-4209-a378-f6b3bda8e80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "alpaca_dataset = load_dataset(\"FreedomIntelligence/alpaca-gpt4-hindi\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd668c4e-abcb-46ee-b230-ba7b45abf275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversations': [{'from': 'human', 'value': '‡§ï‡•Å‡§õ ‡§è‡§ï ‡§∞‡•Ä‡§∏‡§æ‡§á‡§ï‡•ç‡§≤‡§ø‡§Ç‡§ó ‡§Ö‡§≠‡§ø‡§Ø‡§æ‡§® ‡§ï‡•á ‡§≤‡§ø‡§è ‡§è‡§ï ‡§®‡§æ‡§∞‡§æ ‡§∏‡•Å‡§ù‡§æ‡§µ ‡§¶‡•á‡§Ç‡•§\\n'}, {'from': 'gpt', 'value': '1. \"‡§ó‡•ç‡§∞‡•Ä‡§® ‡§≠‡§µ‡§ø‡§∑‡•ç‡§Ø ‡§ï‡•á ‡§≤‡§ø‡§è ‡§è‡§ï ‡§∏‡§æ‡§•: ‡§ï‡§Æ ‡§ï‡§∞‡•á‡§Ç, ‡§™‡•Å‡§®: ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç, ‡§∞‡•Ä‡§∏‡§æ‡§á‡§ï‡§≤ ‡§ï‡§∞‡•á‡§Ç‡•§\"\\n2. \"‡§è‡§ï ‡§¨‡•á‡§π‡§§‡§∞ ‡§ï‡§≤ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§ú ‡§π‡•Ä ‡§∞‡•Ä‡§∏‡§æ‡§á‡§ï‡§≤ ‡§ï‡§∞‡•á‡§Ç‡•§\"\\n3. \"‡§Ö‡§™‡§®‡•á ‡§ï‡§ö‡§∞‡•á ‡§ï‡•ã ‡§ñ‡§ú‡§æ‡§®‡§æ ‡§¨‡§®‡§æ‡§è‡§Ç - ‡§∞‡•Ä‡§∏‡§æ‡§á‡§ï‡§≤ ‡§ï‡§∞‡•á‡§Ç!\"\\n4. \"‡§ú‡•Ä‡§µ‡§® ‡§ï‡•á ‡§ö‡§ï‡•ç‡§∞ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∞‡•Ä‡§∏‡§æ‡§á‡§ï‡§≤ ‡§ï‡§∞‡•á‡§Ç‡•§\"\\n5. \"‡§∏‡§Ç‡§∏‡§æ‡§ß‡§® ‡§¨‡§ö‡§æ‡§è‡§Ç, ‡§Ö‡§ß‡§ø‡§ï ‡§∞‡•Ä‡§∏‡§æ‡§á‡§ï‡§≤ ‡§ï‡§∞‡•á‡§Ç‡•§\"'}], 'id': '23712'}\n"
     ]
    }
   ],
   "source": [
    "print(alpaca_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "652a61a4-26b2-40fb-be90-0aee94a778f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_hindi_prompt=\"\"\"‡§®‡•Ä‡§ö‡•á ‡§è‡§ï ‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂ ‡§π‡•à ‡§ú‡•ã ‡§ï‡§ø‡§∏‡•Ä ‡§ï‡§æ‡§∞‡•ç‡§Ø ‡§ï‡§æ ‡§µ‡§∞‡•ç‡§£‡§® ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, ‡§ú‡§ø‡§∏‡•á ‡§è‡§ï ‡§á‡§®‡§™‡•Å‡§ü ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ú‡•ã‡§°‡§º‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à ‡§ú‡•ã ‡§Ü‡§ó‡•á ‡§ï‡§æ ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ ‡§ê‡§∏‡§æ ‡§â‡§§‡•ç‡§§‡§∞ ‡§≤‡§ø‡§ñ‡•á‡§Ç ‡§ú‡•ã ‡§Ö‡§®‡•Å‡§∞‡•ã‡§ß ‡§ï‡•ã ‡§â‡§ö‡§ø‡§§ ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§™‡•Ç‡§∞‡§æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•ã‡•§\n",
    "\n",
    "### ‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂:\n",
    "{}\n",
    "\n",
    "### ‡§á‡§®‡§™‡•Å‡§ü:\n",
    "{}\n",
    "\n",
    "### ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9e65f32-087f-4790-b485-45a367d4fc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def formatting_prompts_func(conversations):\n",
    "    texts = []\n",
    "    conversations = conversations[\"conversations\"]\n",
    "    for convo in conversations:\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        # Careful Aya Dataset does not have an input!\n",
    "        text = alpaca_hindi_prompt.format(convo[0][\"value\"], \"\", convo[1][\"value\"]) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "alpaca_dataset = alpaca_dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ce475d9-a0f5-429d-91f8-a3ce5ff3e74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.5\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.41 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30f7c7ea3414fdc9527d33b21774582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"lora_unsloth_unopt_full\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d8ca134-1ed3-4cca-b228-5fa6b4dbfae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# Use Lora adapters to update only 1 - 10 % of params\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,   # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eb99789-388f-41d7-9a60-58a944143790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 335,544,320 || all params: 8,365,805,568 || trainable%: 4.0109\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e5c6a29-352e-41eb-892e-9dd96e776b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer_instrc = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = alpaca_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 8,\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "\n",
    "        # Use num_train_epochs and warmup_ratio for longer runs!\n",
    "        max_steps = 240,\n",
    "        warmup_steps = 20,\n",
    "        # warmup_ratio = 0.1,\n",
    "        # num_train_epochs = 1,\n",
    "\n",
    "        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n",
    "        learning_rate = 5e-5,\n",
    "        # embedding_learning_rate = 1e-5,\n",
    "\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 20,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.00,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs_instrc\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74b280a2-e4a5-4336-94fd-a17df3aa03c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4090. Max memory = 23.41 GB.\n",
      "6.719 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cdbd2b8-aa35-417a-8fc1-cb948244baf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 49,969 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 240\n",
      " \"-____-\"     Number of trainable parameters = 335,544,320\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 09:54, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.269200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.938800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.821700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.875600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.832700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.804100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.855900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.781600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.830700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.776100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.815100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.793100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_instrc_stats = trainer_instrc.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e34853a-a54b-48cb-8ddb-cff4937e8d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596.7715 seconds used for training.\n",
      "9.95 minutes used for training.\n",
      "Peak reserved memory = 11.332 GB.\n",
      "Peak reserved memory for training = 4.613 GB.\n",
      "Peak reserved memory % of max memory = 48.407 %.\n",
      "Peak reserved memory for training % of max memory = 19.705 %.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_instrc_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_instrc_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e499a98a-85a9-4ce4-8e04-3581519bdd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_unsloth_instrc_unopt/tokenizer_config.json',\n",
       " 'lora_unsloth_instrc_unopt/special_tokens_map.json',\n",
       " 'lora_unsloth_instrc_unopt/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save final model\n",
    "model.save_pretrained(\"lora_unsloth_instrc_unopt\")\n",
    "tokenizer.save_pretrained(\"lora_unsloth_instrc_unopt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12d7878c-f3fc-44d1-bb57-036987230e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: You are pushing to hub, but you passed your HF username = saucam.\n",
      "We shall truncate saucam/lora-llama-3-8B-unsloth-unopt to lora-llama-3-8B-unsloth-unopt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 76.58 out of 125.5 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 20/32 [00:00<00:00, 58.32it/s]We will save to Disk and not RAM now.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:02<00:00, 11.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e124245a95e4fb3897bd400ae0cfeed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09c3963408e40ae92ecfcf9d72aec0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb56950e446a446f9c19bac155b1b70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc246a8259d9427f8cbbce66d3884377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e18890e977a4404b42a5e59350ed45d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Saved merged model to https://huggingface.co/saucam/lora-llama-3-8B-unsloth-unopt\n"
     ]
    }
   ],
   "source": [
    "# model.push_to_hub_merged(\"saucam/lora-llama-3-8B-unsloth-unopt\", tokenizer, save_method = \"merged_16bit\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41299a22-85a8-41c4-bc1e-a9bedafef3a9",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a95b0cb0-7ef2-46fa-9b20-44a036fdde1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae8d2431-daa6-487d-b80e-cd759638c7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.5\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.41 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cef6d5cf86446359c97f825b7f5b029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"lora_unsloth_instrc_unopt\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# alpaca_prompt = You MUST copy from above!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d613dd9a-75e0-4e47-bbe3-b1914ac46f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>‡§®‡•Ä‡§ö‡•á ‡§è‡§ï ‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂ ‡§π‡•à ‡§ú‡•ã ‡§ï‡§ø‡§∏‡•Ä ‡§ï‡§æ‡§∞‡•ç‡§Ø ‡§ï‡§æ ‡§µ‡§∞‡•ç‡§£‡§® ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, ‡§ú‡§ø‡§∏‡•á ‡§è‡§ï ‡§á‡§®‡§™‡•Å‡§ü ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ú‡•ã‡§°‡§º‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à ‡§ú‡•ã ‡§Ü‡§ó‡•á ‡§ï‡§æ ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ ‡§ê‡§∏‡§æ ‡§â‡§§‡•ç‡§§‡§∞ ‡§≤‡§ø‡§ñ‡•á‡§Ç ‡§ú‡•ã ‡§Ö‡§®‡•Å‡§∞‡•ã‡§ß ‡§ï‡•ã ‡§â‡§ö‡§ø‡§§ ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§™‡•Ç‡§∞‡§æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•ã‡•§\\n\\n### ‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂:\\n‡§™‡•É‡§•‡•ç‡§µ‡•Ä ‡§ó‡•ç‡§∞‡§π ‡§ï‡§æ ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§µ‡§∞‡•ç‡§£‡§® ‡§ï‡§∞‡•á‡§Ç\\n\\n### ‡§á‡§®‡§™‡•Å‡§ü:\\n\\n\\n### ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ:\\n‡§™‡•É‡§•‡•ç‡§µ‡•Ä ‡§ó‡•ç‡§∞‡§π ‡§è‡§ï ‡§ó‡•ç‡§∞‡§π ‡§π‡•à ‡§ú‡•ã ‡§∏‡•Ç‡§∞‡•ç‡§Ø ‡§ï‡•Ä ‡§ï‡§ï‡•ç‡§∑‡§æ ‡§Æ‡•á‡§Ç ‡§∏‡•Ç‡§∞‡•ç‡§Ø ‡§∏‡•á ‡§ï‡•Å‡§õ 150 ‡§Æ‡§ø‡§≤‡§ø‡§Ø‡§® ‡§ï‡§ø‡§≤‡•ã‡§Æ‡•Ä‡§ü‡§∞ ‡§ï‡•Ä ‡§¶‡•Ç‡§∞‡•Ä ‡§™‡§∞ ‡§è‡§ï ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§∏‡•ç‡§•‡§æ‡§® ‡§™‡§∞ ‡§∏‡•ç‡§•‡§ø‡§§ ‡§π‡•à‡•§ ‡§™‡•É‡§•‡•ç‡§µ‡•Ä ‡§ó‡•ç‡§∞‡§π ‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§™‡•ç‡§∞‡§æ‡§ö‡•Ä‡§® ‡§ó‡•ç‡§∞‡•Ä‡§ï ‡§¶‡•á‡§µ‡•Ä ‡§ó‡§æ ‡§ï‡•á ‡§®‡§æ‡§Æ ‡§™‡§∞ ‡§∞‡§ñ‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à, ‡§ú‡•ã ‡§ï‡§ø‡§∏‡•Ä ‡§™‡•ç‡§∞‡§æ‡§•‡§Æ‡§ø‡§ï ‡§¶‡•á‡§µ‡§§‡§æ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§Æ‡§æ‡§®‡•Ä ‡§ú‡§æ‡§§‡•Ä ‡§π‡•à‡•§ ‡§™‡•É‡§•‡•ç‡§µ‡•Ä ‡§ó‡•ç‡§∞‡§π ‡§ï‡§æ ‡§µ‡•ç‡§Ø‡§æ‡§∏ 12,742 ‡§ï‡§ø‡§≤‡•ã‡§Æ‡•Ä‡§ü‡§∞ ‡§π‡•à ‡§î‡§∞ ‡§Ø‡§π ‡§∏‡•Ç‡§∞‡•ç‡§Ø ‡§ï‡•á ‡§™‡§æ‡§∏ ‡§ï‡§ï‡•ç‡§∑‡§æ ‡§ï‡•á ‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•á‡§ï ‡§∏‡•ç‡§•‡§æ‡§® ‡§™‡§∞ ‡§ï‡§ø‡§∏‡•Ä ‡§ó‡•ç‡§∞‡§π ‡§ï‡•á ‡§≤‡§ø‡§è ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§∏‡•ç‡§•‡§æ‡§® ‡§π‡•à‡•§ ‡§™‡•É‡§•‡•ç‡§µ‡•Ä ‡§ó‡•ç‡§∞‡§π ‡§ï‡•Ä ‡§µ‡§ø‡§∂‡•á‡§∑‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§Ø‡§π ‡§è‡§ï ‡§™‡§∞‡•ç‡§Ø‡§æ‡§µ‡§∞‡§£ ‡§Æ‡•á‡§Ç ‡§ú‡•Ä‡§µ‡§® ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ö‡§®‡•Å‡§ï‡•Ç‡§≤ ‡§π‡•à, ‡§ú‡•ã ‡§∏‡•Ç‡§∞‡•ç‡§Ø ‡§ï‡•á ‡§™‡§æ‡§∏ ‡§∏‡•ç‡§•‡§ø‡§§ ‡§ó‡•ç‡§∞‡§π ‡§ï‡•á ‡§≤‡§ø‡§è ‡§è‡§ï ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§∏‡•ç‡§•‡§æ‡§® ‡§π‡•à‡•§ ‡§™‡•É‡§•‡•ç‡§µ']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_hindi_prompt.format(\n",
    "        # \"Describe the planet Earth extensively.\", # instruction\n",
    "        \"‡§™‡•É‡§•‡•ç‡§µ‡•Ä ‡§ó‡•ç‡§∞‡§π ‡§ï‡§æ ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§µ‡§∞‡•ç‡§£‡§® ‡§ï‡§∞‡•á‡§Ç\",\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    ),\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True, repetition_penalty = 0.9, temperature = 0.7)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3170b7-3ab5-4bea-81a9-37f780bc936a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
